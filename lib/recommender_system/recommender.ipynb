{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "import pprint\n",
    "# import json\n",
    "# import gzip\n",
    "# import matplotlib.pyplot as plt\n",
    "# import math\n",
    "# import random\n",
    "# import scipy\n",
    "# import sklearn\n",
    "import tensorflow as tf\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_recommenders as tfrs\n",
    "import tempfile\n",
    "from typing import Dict, Text\n",
    "# from scipy.sparse import csr_matrix\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from scipy.sparse.linalg import svds\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from tensorflow.compat.v1 import metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the metadata\n",
    "# data = []\n",
    "# with gzip.open(\"datasets\\Arts_Crafts_and_Sewing_5.json.gz\") as f:\n",
    "#     for l in f:\n",
    "#         data.append(json.loads(l.strip()))\n",
    "\n",
    "# # total length of list, this number equals total number of products\n",
    "# print(len(data))\n",
    "\n",
    "# # first row of the list\n",
    "# print(data[0])\n",
    "\n",
    "# # convert list into pandas dataframe\n",
    "# df = pd.DataFrame.from_dict(data)\n",
    "# # print(len(df))\n",
    "\n",
    "# # remove rows with NaN values\n",
    "# df3 = df.dropna()\n",
    "\n",
    "# reviewTimeTest = df3.sort_values(by=['reviewTime'], key=pd.to_datetime)\n",
    "# reviewTimeTest\n",
    "\n",
    "# # function to smoothen the distribution of user preference\n",
    "# def smooth_user_preference(x):\n",
    "#     return math.log(1+x, 2)\n",
    "\n",
    "# # counting the total amount of user interactions\n",
    "# users_interactions_count = df3.groupby(['reviewerID', 'asin']).size().groupby('reviewerID').size()\n",
    "# print('# users: %d' % len(users_interactions_count))\n",
    "\n",
    "# #  counting the amount of users with at least 5 interactions\n",
    "# users_with_enough_interactions = users_interactions_count[users_interactions_count >= 5].reset_index()[['reviewerID']]\n",
    "# print('# users with at least 5 interactions: %d' % len(users_with_enough_interactions))\n",
    "\n",
    "\n",
    "# print('# of interactions: %d' % len(df3))\n",
    "\n",
    "# # counting the amount of interactions from users with at least 5 interactions\n",
    "# interactions_from_selected_users = df3.merge(users_with_enough_interactions, \n",
    "#                how = 'right',\n",
    "#                left_on = 'reviewerID',\n",
    "#                right_on = 'reviewerID')\n",
    "# print('# of interactions from users with at least 5 interactions: %d' % len(interactions_from_selected_users))\n",
    "\n",
    "# # counting the amount of unique user-to-item interactions\n",
    "# interactions_full = interactions_from_selected_users \\\n",
    "#                     .groupby(['reviewerID', 'asin'])['overall'].sum() \\\n",
    "#                     .apply(smooth_user_preference).reset_index()\n",
    "# print('# of unique user/item interactions: %d' % len(interactions_full))\n",
    "# interactions_full\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'customer_id': b'8839363',\n",
      "          'helpful_votes': 0,\n",
      "          'marketplace': b'US',\n",
      "          'product_category': b'Tools',\n",
      "          'product_id': b'B0029HJAZ8',\n",
      "          'product_parent': b'733806910',\n",
      "          'product_title': b'Stanley 060864R Folding Sawhorse (2-Pack)',\n",
      "          'review_body': b'These things broke after only a few uses. I called t'\n",
      "                         b'o see if I could get replaced. I was transferred and'\n",
      "                         b\" whoever I talked to, said he couldn't help me.\",\n",
      "          'review_date': b'2015-08-24',\n",
      "          'review_headline': b'Cheaply made.',\n",
      "          'review_id': b'R30ZK5V4C0BJWY',\n",
      "          'star_rating': 1,\n",
      "          'total_votes': 0,\n",
      "          'verified_purchase': 0,\n",
      "          'vine': 1}}\n"
     ]
    }
   ],
   "source": [
    "# Toys Section Importing From amazon_us_reviews dataset\n",
    "tools = (tfds.load(\"amazon_us_reviews/Tools_v1_00\", split='train')\n",
    "            # Cache for efficiency.\n",
    "            .cache(tempfile.NamedTemporaryFile().name))\n",
    "\n",
    "for x in tools.take(1).as_numpy_iterator():\n",
    "    pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools = tools.map(lambda x: {\n",
    "#     \"customer_id\": x[\"customer_id\"],\n",
    "#     \"product_title\": x[\"product_title\"],\n",
    "#     \"product_id\": x[\"product_id\"],\n",
    "# })\n",
    "\n",
    "tools1 = tools.map(lambda x: x[\"data\"])\n",
    "toolsProcessed = tools1.map(lambda x: {\n",
    "    \"customer_id\": x[\"customer_id\"],\n",
    "    \"product_title\": x[\"product_title\"],\n",
    "    \"star_rating\": x[\"star_rating\"]\n",
    "})\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "shuffledTools = toolsProcessed.shuffle(10_000, seed=1, reshuffle_each_iteration=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainNum = 8_000\n",
    "testNum = 2_000\n",
    "\n",
    "train = shuffledTools.take(trainNum)\n",
    "# for x,y,z in train:\n",
    "#     print(x,y,z)\n",
    "\n",
    "test = shuffledTools.skip(trainNum).take(testNum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine Unique Customer IDs and Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "customerID = (toolsProcessed\n",
    "              # Retain only the fields we need.\n",
    "              .map(lambda x: x[\"customer_id\"])\n",
    "              )\n",
    "product = (toolsProcessed \n",
    "           .map(lambda x: x[\"product_title\"])\n",
    "           )\n",
    "\n",
    "uniqueCustomerID = np.unique(np.concatenate(list(customerID.batch(1_000))))\n",
    "uniqueProduct = np.unique(np.concatenate(list(product.batch(1_000))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tfrs.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        embeddingDim = 32\n",
    "\n",
    "        ## Model that represents customers with Matrix Factorization\n",
    "        self.customer_model = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=uniqueCustomerID, mask_token=None),\n",
    "            # Embedding for unknown tokens\n",
    "            tf.keras.layers.Embedding(len(uniqueCustomerID) + 1, embeddingDim)\n",
    "        ])\n",
    "\n",
    "        ## Model that represents products\n",
    "        self.product_model = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=uniqueProduct, mask_token=None),\n",
    "            # Embedding for unknown tokens\n",
    "            tf.keras.layers.Embedding(len(uniqueProduct) + 1, embeddingDim)\n",
    "        ])\n",
    "\n",
    "        # Loss function used to train the models using the Factorized Top-k Method\n",
    "        self.task = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(\n",
    "                candidates=product.batch(128).cache().map(self.product_model)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "            customerEmbeddings = self.customer_model(features[\"customer_id\"])\n",
    "            productEmbeddings = self.product_model(features[\"product_title\"])\n",
    "\n",
    "            return self.task(customerEmbeddings, productEmbeddings, compute_metrics=not training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRate = 0.1\n",
    "\n",
    "model = Model()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=learningRate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fitting and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1/1 [==============================] - 7s 7s/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 71897.9453 - regularization_loss: 0.0000e+00 - total_loss: 71897.9453\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 273ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 71765.5625 - regularization_loss: 0.0000e+00 - total_loss: 71765.5625\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 294ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 71578.5234 - regularization_loss: 0.0000e+00 - total_loss: 71578.5234\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d54de16460>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cachedTrain = train.shuffle(10_000).batch(8192).cache()\n",
    "cachedTest = test.batch(4096).cache()\n",
    "\n",
    "model.fit(cachedTrain, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 207s 207s/step - factorized_top_k/top_1_categorical_accuracy: 5.0000e-04 - factorized_top_k/top_5_categorical_accuracy: 5.0000e-04 - factorized_top_k/top_10_categorical_accuracy: 5.0000e-04 - factorized_top_k/top_50_categorical_accuracy: 5.0000e-04 - factorized_top_k/top_100_categorical_accuracy: 5.0000e-04 - loss: 15201.9600 - regularization_loss: 0.0000e+00 - total_loss: 15201.9600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factorized_top_k/top_1_categorical_accuracy': 0.0005000000237487257,\n",
       " 'factorized_top_k/top_5_categorical_accuracy': 0.0005000000237487257,\n",
       " 'factorized_top_k/top_10_categorical_accuracy': 0.0005000000237487257,\n",
       " 'factorized_top_k/top_50_categorical_accuracy': 0.0005000000237487257,\n",
       " 'factorized_top_k/top_100_categorical_accuracy': 0.0005000000237487257,\n",
       " 'loss': 15201.9599609375,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 15201.9599609375}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(cachedTest, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving Top-K Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy values created to simulate larger dataset\n",
    "toolsWithDummy = tf.data.Dataset.concatenate(\n",
    "    product.batch(4096),\n",
    "    product.batch(4096).repeat(1_000).map(lambda x: tf.zeros_like(x))\n",
    ")\n",
    "\n",
    "toolsWithDummyEmb = tf.data.Dataset.concatenate(\n",
    "    product.batch(4096).map(model.product_model),\n",
    "    product.batch(4096).repeat(1_000)\n",
    "      .map(lambda x: model.product_model(x))\n",
    "      .map(lambda x: x * tf.random.uniform(tf.shape(x)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow_recommenders.layers.factorized_top_k.BruteForce at 0x1d54de597f0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brute_force = tfrs.layers.factorized_top_k.BruteForce(model.customer_model)\n",
    "brute_force.index_from_dataset(\n",
    "    product.batch(128).map(lambda prod: (prod, model.product_model(prod)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top recommendations: [b'SE JT6218 5-Piece Watch Band Link Pin Tool Set'\n",
      " b'SE JT6218 5-Piece Watch Band Link Pin Tool Set'\n",
      " b'SE JT6218 5-Piece Watch Band Link Pin Tool Set']\n"
     ]
    }
   ],
   "source": [
    "# Get predictions for user 42.\n",
    "_, titles = brute_force(np.array([\"8839363\"]), k=3)\n",
    "\n",
    "print(f\"Top recommendations: {titles[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfrs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2376/4101388390.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Override the existing streaming candidate source.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m model.task.factorized_metrics = tfrs.metrics.FactorizedTopK(\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mcandidates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoolsWithDummyEmb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tfrs' is not defined"
     ]
    }
   ],
   "source": [
    "# Override the existing streaming candidate source.\n",
    "model.task.factorized_metrics = tfrs.metrics.FactorizedTopK(\n",
    "    candidates=toolsWithDummyEmb\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to recompile the model for the changes to take effect.\n",
    "model.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%time baseline_result = model.evaluate(test.batch(8192), return_dict=True, verbose=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Out Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12, 4))\n",
    "# plt.barh(corr_similar_count['asin'].head(10),\n",
    "#          abs(corr_similar_count['Correlation'].head(10)), \n",
    "#          align='center',\n",
    "#          color='red')\n",
    "# plt.xlabel(\"Popularity\")\n",
    "# plt.title(\"Top 10 Popular Movies\")\n",
    "# plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactions_train, interactions_test = train_test_split(interactions_full,\n",
    "#                                    stratify=interactions_full['reviewerID'], \n",
    "#                                    test_size=0.20,\n",
    "#                                    random_state=42)\n",
    "\n",
    "# print('# interactions on Train set: %d' % len(interactions_train))\n",
    "# print('# interactions on Test set: %d' % len(interactions_test))\n",
    "\n",
    "# #Indexing by reviewerID to speed up the searches during evaluation\n",
    "# interactions_full_indexed = interactions_full.set_index('reviewerID')\n",
    "# interactions_train_indexed = interactions_train.set_index('reviewerID')\n",
    "# interactions_test_indexed = interactions_test.set_index('reviewerID')\n",
    "\n",
    "# def get_items_interacted(reviewerID, interactions):\n",
    "#     # Get the user's data and merge in the item information.\n",
    "#     interacted_items = interactions.loc[reviewerID]['asin']\n",
    "#     return set(interacted_items if type(interacted_items) == pd.Series else [interacted_items])\n",
    "\n",
    "# # Top-N Accuracy Metrics\n",
    "# rand_non_interacted_items = 200\n",
    "\n",
    "# class ModelEvaluator:\n",
    "#     def get_non_interacted_items_sample(self, reviewerID, sample_size, seed=40):\n",
    "#         interacted_items = get_items_interacted(reviewerID, interactions_full_indexed)\n",
    "#         total_items = set(df3[\"asin\"])\n",
    "#         non_interacted_items = total_items - interacted_items\n",
    "\n",
    "#         random.seed(seed)\n",
    "#         non_interacted_items_sample = random.sample(non_interacted_items, k=sample_size)\n",
    "#         return set(non_interacted_items_sample)\n",
    "\n",
    "#     # def _verify_hit_top_n (self, ):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "tf.metrics.recall_at_k is not supported when eager execution is enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1020/1559968694.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m.10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m.50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m.30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m.04\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m.01\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mrec_at_k\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecall_at_k\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mprec_at_k\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprecision_at_k\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\metrics_impl.py\u001b[0m in \u001b[0;36mrecall_at_k\u001b[1;34m(labels, predictions, k, class_id, weights, metrics_collections, updates_collections, name)\u001b[0m\n\u001b[0;32m   2734\u001b[0m   \"\"\"\n\u001b[0;32m   2735\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2736\u001b[1;33m     raise RuntimeError('tf.metrics.recall_at_k is not '\n\u001b[0m\u001b[0;32m   2737\u001b[0m                        'supported when eager execution is enabled.')\n\u001b[0;32m   2738\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: tf.metrics.recall_at_k is not supported when eager execution is enabled."
     ]
    }
   ],
   "source": [
    "labels = tf.constant([[0, 3]], tf.int64)\n",
    "predictions = tf.constant([[.10, .50, .30, .04, .05, .01]])\n",
    "for k in range(1, 6):\n",
    "    rec_at_k = tf.compat.v1.metrics.recall_at_k(labels, predictions, k)\n",
    "    prec_at_k = tf.compat.v1.metrics.precision_at_k(labels, predictions, k)\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        sess.run([tf.compat.v1.local_variables_initializer(), tf.compat.v1.global_variables_initializer()])\n",
    "        r, p = sess.run([rec_at_k, prec_at_k])\n",
    "        print('recall@{} = {:.2f}, precision@{} = {:.2f}'.format(k, r[1], k, p[1]))\n",
    "\n",
    "# Average Precision@k\n",
    "k = 4\n",
    "labels = tf.constant([[1, 0, 4],\n",
    "                      [0, 0, 0],\n",
    "                      [0, 2, 2]], tf.int64)\n",
    "predictions = tf.constant([[.10, .50, .30, .04, .05, .01],\n",
    "                           [.20, .60, .03, .10, .06, .01],\n",
    "                           [.08, .25, .50, .15, .01, .01]])\n",
    "avg_prec = tf.compat.v1.metrics.average_precision_at_k(labels, predictions, k)\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run([tf.compat.v1.local_variables_initializer(), tf.compat.v1.global_variables_initializer()])\n",
    "    print('{:.4f}'.format(sess.run(avg_prec[1])))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c79836e9502371f3495a2fe8b06d886a0383db19e280854274d759d1837c3622"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
